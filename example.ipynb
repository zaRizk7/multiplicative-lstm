{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mlstm import mLSTM\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "torch.manual_seed(2022)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/gbp-idr.csv\")\n",
    "prices = df.Close.values\n",
    "\n",
    "\n",
    "def sliding_window(data, window_size=4, stride=1):\n",
    "    return np.stack(\n",
    "        [\n",
    "            data[i : i + window_size]\n",
    "            for i in range(0, data.shape[0] - window_size + 1, stride)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = sliding_window(prices, 257).astype(np.float32)\n",
    "inputs, targets = window[:, :-1], window[:, -1]\n",
    "\n",
    "size = inputs.shape[0]\n",
    "while size % 128 > 0:\n",
    "    size -= 1\n",
    "\n",
    "train_size = size * 80 // 100\n",
    "\n",
    "while train_size % 128 > 0:\n",
    "    train_size -= 1\n",
    "\n",
    "inputs_train, targets_train = inputs[:train_size], targets[:train_size]\n",
    "inputs_eval, targets_eval = inputs[train_size:], targets[train_size:]\n",
    "eval_size = inputs_eval.shape[0] // 2\n",
    "\n",
    "inputs_valid, targets_valid = inputs_eval[eval_size:], targets_eval[eval_size:]\n",
    "inputs_test, targets_test = inputs_eval[:eval_size], targets_eval[:eval_size]\n",
    "\n",
    "valid_size = inputs_valid.shape[0]\n",
    "\n",
    "while valid_size % 128 > 0:\n",
    "    valid_size -= 1\n",
    "\n",
    "inputs_valid, targets_valid = inputs_eval[:valid_size], targets_eval[:valid_size]\n",
    "\n",
    "test_size = inputs_valid.shape[0]\n",
    "\n",
    "while test_size % 128 > 0:\n",
    "    test_size -= 1\n",
    "\n",
    "inputs_valid, targets_valid = inputs_eval[:test_size], targets_eval[:test_size]\n",
    "\n",
    "inputs_train, targets_train = torch.from_numpy(inputs_train), torch.from_numpy(\n",
    "    targets_train\n",
    ")\n",
    "inputs_valid, targets_valid = torch.from_numpy(inputs_valid), torch.from_numpy(\n",
    "    targets_valid\n",
    ")\n",
    "inputs_test, targets_test = torch.from_numpy(inputs_test), torch.from_numpy(\n",
    "    targets_test\n",
    ")\n",
    "\n",
    "mean, std = inputs_train.mean(0), inputs_train.std(0)\n",
    "\n",
    "dataset_train = torch.utils.data.TensorDataset(\n",
    "    (inputs_train - mean) / std, targets_train\n",
    ")\n",
    "dataset_valid = torch.utils.data.TensorDataset(\n",
    "    (inputs_valid - mean) / std, targets_valid\n",
    ")\n",
    "dataset_test = torch.utils.data.TensorDataset((inputs_test - mean) / std, targets_test)\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, 128)\n",
    "dataloader_valid = torch.utils.data.DataLoader(dataset_valid, 128)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_mlstm=1):\n",
    "        super().__init__()\n",
    "        self.mlstm = mLSTM(input_size, hidden_size, n_mlstm)\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.25), torch.nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, hidden_states=None):\n",
    "        _, (h_n, c_n) = self.mlstm(inputs, hidden_states)\n",
    "\n",
    "        return (h_n, c_n), self.regressor(h_n[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Training Loss: 14522.623453776041\n",
      "Validation Loss: 14784.06103515625\n",
      "\n",
      "Epoch 2/100\n",
      "Training Loss: 11353.149993896484\n",
      "Validation Loss: 11634.990397135416\n",
      "\n",
      "Epoch 3/100\n",
      "Training Loss: 8453.906885782877\n",
      "Validation Loss: 8394.091634114584\n",
      "\n",
      "Epoch 4/100\n",
      "Training Loss: 9146.588321685791\n",
      "Validation Loss: 10695.666341145834\n",
      "\n",
      "Epoch 5/100\n",
      "Training Loss: 14041.235088348389\n",
      "Validation Loss: 6998.41015625\n",
      "\n",
      "Epoch 6/100\n",
      "Training Loss: 10853.359436035156\n",
      "Validation Loss: 3323.6253662109375\n",
      "\n",
      "Epoch 7/100\n",
      "Training Loss: 8199.039396921793\n",
      "Validation Loss: 830.0427398681641\n",
      "\n",
      "Epoch 8/100\n",
      "Training Loss: 6815.795986175537\n",
      "Validation Loss: 901.8446807861328\n",
      "\n",
      "Epoch 9/100\n",
      "Training Loss: 5592.1659418741865\n",
      "Validation Loss: 907.3913523356119\n",
      "\n",
      "Epoch 10/100\n",
      "Training Loss: 4446.1663004557295\n",
      "Validation Loss: 923.8132069905599\n",
      "\n",
      "Epoch 11/100\n",
      "Training Loss: 3437.0196380615234\n",
      "Validation Loss: 915.4478098551432\n",
      "\n",
      "Epoch 12/100\n",
      "Training Loss: 2907.88316599528\n",
      "Validation Loss: 945.8363444010416\n",
      "\n",
      "Epoch 13/100\n",
      "Training Loss: 2843.5716298421225\n",
      "Validation Loss: 940.2707316080729\n",
      "\n",
      "Epoch 14/100\n",
      "Training Loss: 2835.637727101644\n",
      "Validation Loss: 939.1712086995443\n",
      "\n",
      "Epoch 15/100\n",
      "Training Loss: 2817.2986653645835\n",
      "Validation Loss: 949.2413635253906\n",
      "\n",
      "Epoch 16/100\n",
      "Training Loss: 2802.1337610880532\n",
      "Validation Loss: 982.0255635579427\n",
      "\n",
      "Epoch 17/100\n",
      "Training Loss: 2793.87334950765\n",
      "Validation Loss: 973.1063588460287\n",
      "\n",
      "Epoch 18/100\n",
      "Training Loss: 2785.1814511617026\n",
      "Validation Loss: 980.6764068603516\n",
      "\n",
      "Epoch 19/100\n",
      "Training Loss: 2768.5328890482583\n",
      "Validation Loss: 992.9541727701823\n",
      "\n",
      "Epoch 20/100\n",
      "Training Loss: 2758.7768128712974\n",
      "Validation Loss: 991.5576121012369\n",
      "\n",
      "Epoch 21/100\n",
      "Training Loss: 2759.282159169515\n",
      "Validation Loss: 996.3489685058594\n",
      "\n",
      "Epoch 22/100\n",
      "Training Loss: 2738.2638918558755\n",
      "Validation Loss: 1000.3991851806641\n",
      "\n",
      "Epoch 23/100\n",
      "Training Loss: 2743.6229248046875\n",
      "Validation Loss: 1003.0600840250651\n",
      "\n",
      "Epoch 24/100\n",
      "Training Loss: 2753.3117071787515\n",
      "Validation Loss: 1003.9717152913412\n",
      "\n",
      "Epoch 25/100\n",
      "Training Loss: 2747.367689768473\n",
      "Validation Loss: 1004.2544759114584\n",
      "\n",
      "Epoch 26/100\n",
      "Training Loss: 2748.5063756306968\n",
      "Validation Loss: 1003.5082041422526\n",
      "\n",
      "Epoch 27/100\n",
      "Training Loss: 2730.9644883473716\n",
      "Validation Loss: 1001.9254302978516\n",
      "\n",
      "Epoch 28/100\n",
      "Training Loss: 2751.933515548706\n",
      "Validation Loss: 999.3207041422526\n",
      "\n",
      "Epoch 29/100\n",
      "Training Loss: 2747.2295220692954\n",
      "Validation Loss: 998.794443766276\n",
      "\n",
      "Epoch 30/100\n",
      "Training Loss: 2749.807724634806\n",
      "Validation Loss: 990.7804616292318\n",
      "\n",
      "Epoch 31/100\n",
      "Training Loss: 2740.8937288920083\n",
      "Validation Loss: 984.4867146809896\n",
      "\n",
      "Epoch 32/100\n",
      "Training Loss: 2724.9509862264\n",
      "Validation Loss: 981.8280639648438\n",
      "\n",
      "Epoch 33/100\n",
      "Training Loss: 2740.554414113363\n",
      "Validation Loss: 983.0496520996094\n",
      "\n",
      "Epoch 34/100\n",
      "Training Loss: 2737.4896659851074\n",
      "Validation Loss: 996.3919423421224\n",
      "\n",
      "Epoch 35/100\n",
      "Training Loss: 2717.6246089935303\n",
      "Validation Loss: 987.4074045817057\n",
      "\n",
      "Epoch 36/100\n",
      "Training Loss: 2723.8048254648843\n",
      "Validation Loss: 980.5228830973307\n",
      "\n",
      "Epoch 37/100\n",
      "Training Loss: 2707.4053757985434\n",
      "Validation Loss: 1008.9444427490234\n",
      "\n",
      "Epoch 38/100\n",
      "Training Loss: 2687.629037221273\n",
      "Validation Loss: 963.9996287027994\n",
      "\n",
      "Epoch 39/100\n",
      "Training Loss: 2691.008066813151\n",
      "Validation Loss: 980.6425272623698\n",
      "\n",
      "Epoch 40/100\n",
      "Training Loss: 2687.8355547587075\n",
      "Validation Loss: 1004.996083577474\n",
      "\n",
      "Epoch 41/100\n",
      "Training Loss: 2652.208168029785\n",
      "Validation Loss: 990.5302225748698\n",
      "\n",
      "Epoch 42/100\n",
      "Training Loss: 2641.2262338002524\n",
      "Validation Loss: 1011.9117889404297\n",
      "\n",
      "Epoch 43/100\n",
      "Training Loss: 2643.6230659484863\n",
      "Validation Loss: 1048.1385142008464\n",
      "\n",
      "Epoch 44/100\n",
      "Training Loss: 2635.9698651631675\n",
      "Validation Loss: 1024.6912841796875\n",
      "\n",
      "Epoch 45/100\n",
      "Training Loss: 2609.607000986735\n",
      "Validation Loss: 1032.7145741780598\n",
      "\n",
      "Epoch 46/100\n",
      "Training Loss: 2612.40203221639\n",
      "Validation Loss: 1047.0021362304688\n",
      "\n",
      "Epoch 47/100\n",
      "Training Loss: 2600.8523960113525\n",
      "Validation Loss: 1064.0236409505208\n",
      "\n",
      "Epoch 48/100\n",
      "Training Loss: 2580.0910453796387\n",
      "Validation Loss: 1133.489537556966\n",
      "\n",
      "Epoch 49/100\n",
      "Training Loss: 2573.150187810262\n",
      "Validation Loss: 1151.932393391927\n",
      "\n",
      "Epoch 50/100\n",
      "Training Loss: 2569.005911509196\n",
      "Validation Loss: 1139.846227010091\n",
      "\n",
      "Epoch 51/100\n",
      "Training Loss: 2536.3294626871743\n",
      "Validation Loss: 1152.2877960205078\n",
      "\n",
      "Epoch 52/100\n",
      "Training Loss: 2540.469612121582\n",
      "Validation Loss: 1174.9392801920574\n",
      "\n",
      "Epoch 53/100\n",
      "Training Loss: 2523.316608428955\n",
      "Validation Loss: 1197.094223022461\n",
      "\n",
      "Epoch 54/100\n",
      "Training Loss: 2502.6004638671875\n",
      "Validation Loss: 1245.4478302001953\n",
      "\n",
      "Epoch 55/100\n",
      "Training Loss: 2497.5801378885903\n",
      "Validation Loss: 1287.977778116862\n",
      "\n",
      "Epoch 56/100\n",
      "Training Loss: 2480.0346234639487\n",
      "Validation Loss: 1269.7547047932942\n",
      "\n",
      "Epoch 57/100\n",
      "Training Loss: 2471.008622487386\n",
      "Validation Loss: 1338.8266092936199\n",
      "\n",
      "Epoch 58/100\n",
      "Training Loss: 2437.0940729777017\n",
      "Validation Loss: 1321.8595581054688\n",
      "\n",
      "Epoch 59/100\n",
      "Training Loss: 2420.592627843221\n",
      "Validation Loss: 1309.0240580240886\n",
      "\n",
      "Epoch 60/100\n",
      "Training Loss: 2411.8261839548745\n",
      "Validation Loss: 1373.8986104329426\n",
      "\n",
      "Epoch 61/100\n",
      "Training Loss: 2406.7472553253174\n",
      "Validation Loss: 1371.7352803548176\n",
      "\n",
      "Epoch 62/100\n",
      "Training Loss: 2388.968413670858\n",
      "Validation Loss: 1382.6470235188801\n",
      "\n",
      "Epoch 63/100\n",
      "Training Loss: 2363.1124884287515\n",
      "Validation Loss: 1394.084696451823\n",
      "\n",
      "Epoch 64/100\n",
      "Training Loss: 2377.9505195617676\n",
      "Validation Loss: 1416.0199279785156\n",
      "\n",
      "Epoch 65/100\n",
      "Training Loss: 2351.6948216756186\n",
      "Validation Loss: 1445.6233723958333\n",
      "\n",
      "Epoch 66/100\n",
      "Training Loss: 2345.6261253356934\n",
      "Validation Loss: 1470.9568379720051\n",
      "\n",
      "Epoch 67/100\n",
      "Training Loss: 2338.8289229075112\n",
      "Validation Loss: 1458.6673990885417\n",
      "\n",
      "Epoch 68/100\n",
      "Training Loss: 2334.304902394613\n",
      "Validation Loss: 1474.0432637532551\n",
      "\n",
      "Epoch 69/100\n",
      "Training Loss: 2318.573632558187\n",
      "Validation Loss: 1475.3669026692708\n",
      "\n",
      "Epoch 70/100\n",
      "Training Loss: 2326.130247116089\n",
      "Validation Loss: 1470.9432271321614\n",
      "\n",
      "Epoch 71/100\n",
      "Training Loss: 2326.749190012614\n",
      "Validation Loss: 1475.6536356608074\n",
      "\n",
      "Epoch 72/100\n",
      "Training Loss: 2308.6615365346274\n",
      "Validation Loss: 1471.2571716308594\n",
      "\n",
      "Epoch 73/100\n",
      "Training Loss: 2304.95120938619\n",
      "Validation Loss: 1472.5794576009114\n",
      "\n",
      "Epoch 74/100\n",
      "Training Loss: 2317.025588353475\n",
      "Validation Loss: 1473.0949198404949\n",
      "\n",
      "Epoch 75/100\n",
      "Training Loss: 2306.963925679525\n",
      "Validation Loss: 1473.0502217610676\n",
      "\n",
      "Epoch 76/100\n",
      "Training Loss: 2307.0122776031494\n",
      "Validation Loss: 1472.2380676269531\n",
      "\n",
      "Epoch 77/100\n",
      "Training Loss: 2301.157791773478\n",
      "Validation Loss: 1470.0062357584636\n",
      "\n",
      "Epoch 78/100\n",
      "Training Loss: 2313.801700592041\n",
      "Validation Loss: 1466.4375813802083\n",
      "\n",
      "Epoch 79/100\n",
      "Training Loss: 2316.1489054361978\n",
      "Validation Loss: 1460.0858561197917\n",
      "\n",
      "Epoch 80/100\n",
      "Training Loss: 2314.8465258280435\n",
      "Validation Loss: 1459.6325581868489\n",
      "\n",
      "Epoch 81/100\n",
      "Training Loss: 2314.275458017985\n",
      "Validation Loss: 1445.5930582682292\n",
      "\n",
      "Epoch 82/100\n",
      "Training Loss: 2329.9023106892905\n",
      "Validation Loss: 1434.6345621744792\n",
      "\n",
      "Epoch 83/100\n",
      "Training Loss: 2327.101287841797\n",
      "Validation Loss: 1446.5691426595051\n",
      "\n",
      "Epoch 84/100\n",
      "Training Loss: 2319.504767100016\n",
      "Validation Loss: 1451.729715983073\n",
      "\n",
      "Epoch 85/100\n",
      "Training Loss: 2320.6798178354898\n",
      "Validation Loss: 1481.653564453125\n",
      "\n",
      "Epoch 86/100\n",
      "Training Loss: 2320.52321434021\n",
      "Validation Loss: 1477.4826049804688\n",
      "\n",
      "Epoch 87/100\n",
      "Training Loss: 2326.4307791392007\n",
      "Validation Loss: 1482.8727315266926\n",
      "\n",
      "Epoch 88/100\n",
      "Training Loss: 2324.7522271474204\n",
      "Validation Loss: 1512.0548095703125\n",
      "\n",
      "Epoch 89/100\n",
      "Training Loss: 2330.354017893473\n",
      "Validation Loss: 1484.1793823242188\n",
      "\n",
      "Epoch 90/100\n",
      "Training Loss: 2317.7762769063315\n",
      "Validation Loss: 1530.6694030761719\n",
      "\n",
      "Epoch 91/100\n",
      "Training Loss: 2330.155246734619\n",
      "Validation Loss: 1562.2260437011719\n",
      "\n",
      "Epoch 92/100\n",
      "Training Loss: 2321.2282695770264\n",
      "Validation Loss: 1542.3979187011719\n",
      "\n",
      "Epoch 93/100\n",
      "Training Loss: 2316.2788047790527\n",
      "Validation Loss: 1552.2742207845051\n",
      "\n",
      "Epoch 94/100\n",
      "Training Loss: 2330.2851346333823\n",
      "Validation Loss: 1653.9428609212239\n",
      "\n",
      "Epoch 95/100\n",
      "Training Loss: 2313.8852475484214\n",
      "Validation Loss: 1579.7169494628906\n",
      "\n",
      "Epoch 96/100\n",
      "Training Loss: 2299.8917338053384\n",
      "Validation Loss: 1581.5301005045574\n",
      "\n",
      "Epoch 97/100\n",
      "Training Loss: 2307.6742598215737\n",
      "Validation Loss: 1651.9383036295574\n",
      "\n",
      "Epoch 98/100\n",
      "Training Loss: 2302.9382139841714\n",
      "Validation Loss: 1661.3185119628906\n",
      "\n",
      "Epoch 99/100\n",
      "Training Loss: 2312.2502670288086\n",
      "Validation Loss: 1720.4955749511719\n",
      "\n",
      "Epoch 100/100\n",
      "Training Loss: 2295.067762374878\n",
      "Validation Loss: 1652.4532775878906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = TimeSeriesModel(256, 512).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, len(dataloader_train) * 25\n",
    ")\n",
    "\n",
    "criterion = torch.nn.HuberLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    losses_train, losses_valid = [], []\n",
    "    hidden_state = None\n",
    "    model.train()\n",
    "    for inputs, targets in dataloader_train:\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        (h_n, c_n), predictions = model(inputs, hidden_state)\n",
    "        hidden_state = h_n[-1], c_n[-1]\n",
    "        loss = criterion(predictions.squeeze(1), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses_train.append(loss.detach().item())\n",
    "\n",
    "    hidden_state = None\n",
    "    model.eval()\n",
    "    for inputs, targets in dataloader_valid:\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            (h_n, c_n), predictions = model(inputs, hidden_state)\n",
    "            hidden_state = h_n[-1], c_n[-1]\n",
    "            loss = criterion(predictions.squeeze(1), targets)\n",
    "\n",
    "        losses_valid.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/100\")\n",
    "    print(f\"Training Loss: {np.mean(losses_train)}\")\n",
    "    print(f\"Validation Loss: {np.mean(losses_valid)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
